{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "import numpy as np\n",
    "import PIL\n",
    "import torch\n",
    "from PIL import Image\n",
    "from torch.utils.data import DataLoader, Dataset, SubsetRandomSampler\n",
    "from torchvision import transforms\n",
    "from tqdm import tqdm\n",
    "import argparse\n",
    "from datetime import datetime\n",
    "    \n",
    "# import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import segmentation_models_pytorch as smp\n",
    "import torch\n",
    "from dataset import *\n",
    "from model_utils import *\n",
    "from tqdm import tqdm\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transformations\n",
    "transformation = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.Resize((1024 // 4, 1536 // 4)),\n",
    "        transforms.ToTensor(),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SegmentationDataset(Dataset):\n",
    "    def __init__(self, images, masks, num_classes: int, transforms=None):\n",
    "        self.images = images\n",
    "        self.masks = masks\n",
    "        self.num_classes = num_classes\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.images[idx]\n",
    "\n",
    "        if self.transforms:\n",
    "            image = self.transforms(image)\n",
    "\n",
    "        mask = self.masks[idx]\n",
    "\n",
    "        # - the problem was ToTensor was destroying the class index for the labels (rounding them to 0-1)\n",
    "        # need to to transformation manually\n",
    "        mask = Image.fromarray(mask).resize(\n",
    "            (1536 // 4, 1024 // 4), resample=PIL.Image.NEAREST\n",
    "        )\n",
    "        mask = torch.tensor(np.asarray(mask)).unsqueeze(0)\n",
    "\n",
    "        return image, mask\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_images_and_masks_in_path(images_path, masks_path):\n",
    "    images = []\n",
    "    masks = []\n",
    "    sorted_img_filenames = sorted(glob.glob(images_path + \".tiff\"))  #[-435:]\n",
    "    sorted_mask_filenames = sorted(glob.glob(masks_path + \".tiff\"))  #[-435:]\n",
    "\n",
    "    for img_fname, mask_fname in tqdm(\n",
    "        list(zip(sorted_img_filenames, sorted_mask_filenames))\n",
    "    ):\n",
    "\n",
    "        image = np.asarray(Image.open(img_fname))\n",
    "        mask = np.asarray(Image.open(mask_fname))\n",
    "\n",
    "        images.append(image)\n",
    "        masks.append(mask)\n",
    "    return np.array(images), np.array(masks)\n",
    "\n",
    "\n",
    "def preprocess_data(data_path, num_classes=3, batch_size=25, val_size=0.2):\n",
    "\n",
    "    img_path = f\"{data_path}/train/**/img\"\n",
    "    label_path = f\"{data_path}/train/**/label\"\n",
    "    print(f\"Loading dataset from {img_path}\")\n",
    "\n",
    "    train_images, train_masks = load_images_and_masks_in_path(img_path, label_path)\n",
    "\n",
    "    # load dataset\n",
    "    seg_dataset = SegmentationDataset(\n",
    "        train_images, train_masks, num_classes, transforms=transformation\n",
    "    )\n",
    "\n",
    "    # train/validation splits\n",
    "    dataset_size = len(seg_dataset)\n",
    "    dataset_idx = list(range(dataset_size))\n",
    "    split_idx = int(np.floor(val_size * dataset_size))\n",
    "    train_idx = dataset_idx[split_idx:]\n",
    "    val_idx = dataset_idx[:split_idx]\n",
    "\n",
    "    train_sampler = SubsetRandomSampler(train_idx)\n",
    "    val_sampler = SubsetRandomSampler(val_idx)\n",
    "\n",
    "    train_data_loader = DataLoader(\n",
    "        seg_dataset, batch_size=batch_size, sampler=train_sampler\n",
    "    )  # shuffle=True,\n",
    "    print(f\"Train dataset has {len(train_data_loader)} batches of size {batch_size}\")\n",
    "\n",
    "    val_data_loader = DataLoader(\n",
    "        seg_dataset, batch_size=batch_size, sampler=val_sampler\n",
    "    )  # shuffle=True,\n",
    "    print(f\"Validation dataset has {len(val_data_loader)} batches of size {batch_size}\")\n",
    "\n",
    "    return train_data_loader, val_data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def save_model(model, epoch):\n",
    "#     \"\"\"Helper function for saving the model based on current time and epoch\"\"\"\n",
    "    \n",
    "#     # datetime object containing current date and time\n",
    "#     now = datetime.now()\n",
    "#     # format\n",
    "#     dt_string = now.strftime(\"%d_%m_%Y_%H_%M_%S\") + f\"_n{epoch+1:02d}\"\n",
    "#     model_save_file = f\"models/{dt_string}_model.pt\"\n",
    "#     torch.save(model.state_dict(), model_save_file)\n",
    "\n",
    "#     print(f\"Model saved to {model_save_file}\")\n",
    "\n",
    "# def train_model(model, device, train_data_loader, val_data_loader, epochs, DEBUG=False):\n",
    "#     \"\"\" Helper function for training the model \"\"\"\n",
    "#     # initialise loss function and optimizer\n",
    "#     criterion = torch.nn.CrossEntropyLoss()\n",
    "#     optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "#     total_steps = len(train_data_loader)\n",
    "#     print(f\"{epochs} epochs, {total_steps} total_steps per epoch\")\n",
    "\n",
    "#     # accounting\n",
    "#     train_losses = []\n",
    "#     val_losses = []\n",
    "\n",
    "#     # training loop\n",
    "#     for epoch in tqdm(range(epochs)):\n",
    "#         print(f\"------- Epoch {epoch+1} of {epochs}  --------\")\n",
    "        \n",
    "#         train_loss = 0\n",
    "#         val_loss = 0\n",
    "        \n",
    "#         data_loader = tqdm(train_data_loader)\n",
    "\n",
    "#         for i, (images, masks) in enumerate(data_loader):\n",
    "\n",
    "#             # set model to training mode\n",
    "#             model.train()\n",
    "\n",
    "#             # move img and mask to device, reshape mask\n",
    "#             images = images.to(device)\n",
    "#             masks = masks.type(torch.LongTensor)\n",
    "#             masks = masks.reshape(\n",
    "#                 masks.shape[0], masks.shape[2], masks.shape[3]\n",
    "#             )  # remove channel dim\n",
    "#             masks = masks.to(device)\n",
    "\n",
    "#             # forward pass\n",
    "#             outputs = model(images).type(torch.FloatTensor).to(device)\n",
    "#             loss = criterion(outputs, masks)\n",
    "\n",
    "#             # backwards pass\n",
    "#             optimizer.zero_grad()\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "\n",
    "#             # evaluation\n",
    "#             train_loss += loss.item()\n",
    "#             wandb.log({\"train_loss\": loss.item()})\n",
    "#             data_loader.set_description(f\"Train Loss: {loss.item():.04f}\")\n",
    "\n",
    "#             if i % 100 == 0:\n",
    "          \n",
    "#                 if DEBUG:\n",
    "#                     model.eval()\n",
    "#                     with torch.no_grad():\n",
    "\n",
    "#                         outputs = model(images)\n",
    "#                         output_mask = decode_output(outputs)\n",
    "                        \n",
    "#                         img_base = images.detach().cpu().squeeze().numpy()\n",
    "#                         img_rgb = np.dstack((img_base, img_base, img_base))\n",
    "#                         gt_base = decode_segmap(masks.detach().cpu().permute(1, 2, 0))\n",
    "\n",
    "#                         wb_img = wandb.Image(img_rgb, caption=\"Input Image\")\n",
    "#                         wb_gt = wandb.Image(gt_base, caption=\"Ground Truth\")\n",
    "#                         wb_mask = wandb.Image(output_mask, caption=\"Output Mask\")\n",
    "#                         wandb.log({\"image\": wb_img, \"mask\": wb_mask, \"ground_truth\": wb_gt})\n",
    "                           \n",
    "        \n",
    "#         val_loader = tqdm(val_data_loader)\n",
    "#         for i, (images, masks) in enumerate(val_loader):\n",
    "            \n",
    "#             model.eval()\n",
    "            \n",
    "#             # move img and mask to device, reshape mask\n",
    "#             images = images.to(device)\n",
    "#             masks = masks.type(torch.LongTensor)\n",
    "#             masks = masks.reshape(\n",
    "#                 masks.shape[0], masks.shape[2], masks.shape[3]\n",
    "#             )  # remove channel dim\n",
    "#             masks = masks.to(device)\n",
    "\n",
    "#             # forward pass\n",
    "#             outputs = model(images).type(torch.FloatTensor).to(device)\n",
    "#             loss = criterion(outputs, masks)\n",
    "\n",
    "#             val_loss += loss.item()\n",
    "#             wandb.log({\"val_loss\": loss.item()})\n",
    "#             val_loader.set_description(f\"Val Loss: {loss.item():.04f}\")\n",
    "\n",
    "#         train_losses.append(train_loss / len(train_data_loader))\n",
    "#         val_losses.append(val_loss / len(val_data_loader))\n",
    "\n",
    "#         # save model checkpoint\n",
    "#         save_model(model, epoch)\n",
    "\n",
    "\n",
    "#     return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, epoch):\n",
    "    \"\"\"Helper function for saving the model based on current time and epoch\"\"\"\n",
    "    \n",
    "    # datetime object containing current date and time\n",
    "    now = datetime.now()\n",
    "    # format\n",
    "    dt_string = now.strftime(\"%d_%m_%Y_%H_%M_%S\") + f\"_n{epoch+1:02d}\"\n",
    "    model_save_file = f\"models/{dt_string}_model.pt\"\n",
    "    torch.save(model.state_dict(), model_save_file)\n",
    "\n",
    "    print(f\"Model saved to {model_save_file}\")\n",
    "\n",
    "def train(model, device, data_loader, criterion, optimizer, DEBUG, WANDB):\n",
    "    data_loader = tqdm(data_loader)\n",
    "    train_loss = 0\n",
    "\n",
    "    for i, (images, masks) in enumerate(data_loader):\n",
    "        # set model to training mode\n",
    "        model.train()\n",
    "\n",
    "        # move img and mask to device, reshape mask\n",
    "        images = images.to(device)\n",
    "        masks = masks.type(torch.LongTensor)\n",
    "        masks = masks.reshape(\n",
    "            masks.shape[0], masks.shape[2], masks.shape[3]\n",
    "        )  # remove channel dim\n",
    "        masks = masks.to(device)\n",
    "\n",
    "        # forward pass\n",
    "        outputs = model(images).type(torch.FloatTensor).to(device)\n",
    "        loss = criterion(outputs, masks)\n",
    "\n",
    "        # backwards pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # evaluation\n",
    "        train_loss += loss.item()\n",
    "        if WANDB:\n",
    "            wandb.log({\"train_loss\": loss.item()})\n",
    "        data_loader.set_description(f\"Train Loss: {loss.item():.04f}\")\n",
    "\n",
    "        if i % 100 == 0:\n",
    "        \n",
    "            if DEBUG and WANDB:\n",
    "                model.eval()\n",
    "                with torch.no_grad():\n",
    "\n",
    "                    outputs = model(images)\n",
    "                    output_mask = decode_output(outputs)\n",
    "                    \n",
    "                    img_base = images.detach().cpu().squeeze().numpy()\n",
    "                    img_rgb = np.dstack((img_base, img_base, img_base))\n",
    "                    gt_base = decode_segmap(masks.detach().cpu().permute(1, 2, 0))\n",
    "\n",
    "                    wb_img = wandb.Image(img_rgb, caption=\"Input Image\")\n",
    "                    wb_gt = wandb.Image(gt_base, caption=\"Ground Truth\")\n",
    "                    wb_mask = wandb.Image(output_mask, caption=\"Output Mask\")\n",
    "                    wandb.log({\"image\": wb_img, \"mask\": wb_mask, \"ground_truth\": wb_gt})\n",
    "    \n",
    "    return train_loss\n",
    "\n",
    "def validate(model, device, data_loader, criterion, WANDB):\n",
    "    val_loader = tqdm(data_loader)\n",
    "    val_loss = 0\n",
    "\n",
    "    for i, (images, masks) in enumerate(val_loader):\n",
    "        \n",
    "        model.eval()\n",
    "        \n",
    "        # move img and mask to device, reshape mask\n",
    "        images = images.to(device)\n",
    "        masks = masks.type(torch.LongTensor)\n",
    "        masks = masks.reshape(\n",
    "            masks.shape[0], masks.shape[2], masks.shape[3]\n",
    "        )  # remove channel dim\n",
    "        masks = masks.to(device)\n",
    "\n",
    "        # forward pass\n",
    "        outputs = model(images).type(torch.FloatTensor).to(device)\n",
    "        loss = criterion(outputs, masks)\n",
    "\n",
    "        val_loss += loss.item()\n",
    "        if WANDB:\n",
    "            wandb.log({\"val_loss\": loss.item()})\n",
    "            val_loader.set_description(f\"Val Loss: {loss.item():.04f}\")\n",
    "\n",
    "    return val_loss\n",
    "\n",
    "def train_model(model, device, train_data_loader, val_data_loader, epochs, DEBUG=True, WANDB=True):\n",
    "    \"\"\" Helper function for training the model \"\"\"\n",
    "    # initialise loss function and optimizer\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "    total_steps = len(train_data_loader)\n",
    "    print(f\"{epochs} epochs, {total_steps} total_steps per epoch\")\n",
    "\n",
    "    # accounting\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "\n",
    "    # training loop\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        print(f\"------- Epoch {epoch+1} of {epochs}  --------\")\n",
    "        \n",
    "        train_loss = train(model, device, train_data_loader, criterion, optimizer, DEBUG, WANDB)\n",
    "        val_loss = validate(model, device, val_data_loader, criterion, WANDB)\n",
    "   \n",
    "        train_losses.append(train_loss / len(train_data_loader))\n",
    "        val_losses.append(val_loss / len(val_data_loader))\n",
    "\n",
    "        # save model checkpoint\n",
    "        save_model(model, epoch)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------------- Loading and Preparing Data -----------------------\n",
      "validating dataset...\n",
      "1227 images, 1227 labels\n",
      "finished validating dataset.\n",
      "Loading dataset from /home/ubuntu/same_size/ of length 1227\n",
      "Train dataset has 982 batches of size 1\n",
      "Validation dataset has 245 batches of size 1\n",
      "\n",
      "----------------------- Data Preprocessing Completed -----------------------\n",
      "\n",
      "----------------------- Loading Model -----------------------\n",
      "\n",
      "----------------------- Begin Sanity Check -----------------------\n",
      "\n",
      "imgs, masks, output\n",
      "torch.Size([1, 1, 1536, 1024]) torch.Size([1, 1, 1536, 1024]) torch.Size([1, 3, 1536, 1024])\n",
      "imgs, masks, output\n",
      "torch.Size([1, 1, 1536, 1024]) torch.Size([1, 1, 1536, 1024]) torch.Size([1, 3, 1536, 1024])\n",
      "\n",
      "----------------------- Begin Training -----------------------\n",
      "\n",
      "8 epochs, 982 total_steps per epoch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/8 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- Epoch 1 of 8  --------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0231: 100%|██████████| 982/982 [01:39<00:00,  9.82it/s]\n",
      "100%|██████████| 245/245 [00:10<00:00, 22.78it/s]\n",
      " 12%|█▎        | 1/8 [01:50<12:55, 110.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to models/03_10_2022_15_06_42_n01_model.pt\n",
      "------- Epoch 2 of 8  --------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0138: 100%|██████████| 982/982 [01:35<00:00, 10.30it/s]\n",
      "100%|██████████| 245/245 [00:11<00:00, 21.86it/s]\n",
      " 25%|██▌       | 2/8 [03:37<10:50, 108.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to models/03_10_2022_15_08_28_n02_model.pt\n",
      "------- Epoch 3 of 8  --------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0354:  76%|███████▌  | 743/982 [01:13<00:23, 10.13it/s]\n",
      " 25%|██▌       | 2/8 [04:50<14:32, 145.44s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [9], line 76\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m----------------------- Begin Training -----------------------\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     75\u001b[0m \u001b[38;5;66;03m# train model\u001b[39;00m\n\u001b[0;32m---> 76\u001b[0m model \u001b[38;5;241m=\u001b[39m train_model(model, device, train_data_loader, val_data_loader, epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m8\u001b[39m, DEBUG\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, WANDB\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "Cell \u001b[0;32mIn [7], line 108\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, device, train_data_loader, val_data_loader, epochs, DEBUG, WANDB)\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(epochs)):\n\u001b[1;32m    106\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m------- Epoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m  --------\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 108\u001b[0m     train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_data_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mDEBUG\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mWANDB\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    109\u001b[0m     val_loss \u001b[38;5;241m=\u001b[39m validate(model, device, val_data_loader, criterion, WANDB)\n\u001b[1;32m    111\u001b[0m     train_losses\u001b[38;5;241m.\u001b[39mappend(train_loss \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(train_data_loader))\n",
      "Cell \u001b[0;32mIn [7], line 17\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, device, data_loader, criterion, optimizer, DEBUG, WANDB)\u001b[0m\n\u001b[1;32m     14\u001b[0m data_loader \u001b[38;5;241m=\u001b[39m tqdm(data_loader)\n\u001b[1;32m     15\u001b[0m train_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m---> 17\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, (images, masks) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(data_loader):\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;66;03m# set model to training mode\u001b[39;00m\n\u001b[1;32m     19\u001b[0m     model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;66;03m# move img and mask to device, reshape mask\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/fibsem/lib/python3.9/site-packages/tqdm/std.py:1195\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1192\u001b[0m time \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_time\n\u001b[1;32m   1194\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1195\u001b[0m     \u001b[39mfor\u001b[39;00m obj \u001b[39min\u001b[39;00m iterable:\n\u001b[1;32m   1196\u001b[0m         \u001b[39myield\u001b[39;00m obj\n\u001b[1;32m   1197\u001b[0m         \u001b[39m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1198\u001b[0m         \u001b[39m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/fibsem/lib/python3.9/site-packages/torch/utils/data/dataloader.py:681\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    678\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    679\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    680\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 681\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    682\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    683\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    684\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    685\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/miniconda3/envs/fibsem/lib/python3.9/site-packages/torch/utils/data/dataloader.py:721\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    719\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    720\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 721\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    722\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[1;32m    723\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/miniconda3/envs/fibsem/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:49\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfetch\u001b[39m(\u001b[39mself\u001b[39m, possibly_batched_index):\n\u001b[1;32m     48\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mauto_collation:\n\u001b[0;32m---> 49\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/miniconda3/envs/fibsem/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:49\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfetch\u001b[39m(\u001b[39mself\u001b[39m, possibly_batched_index):\n\u001b[1;32m     48\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mauto_collation:\n\u001b[0;32m---> 49\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/fibsem/fibsem/segmentation/dataset.py:30\u001b[0m, in \u001b[0;36mSegmentationDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__getitem__\u001b[39m(\u001b[39mself\u001b[39m, idx):\n\u001b[0;32m---> 30\u001b[0m     image \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39masarray(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mimages[idx])\n\u001b[1;32m     31\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransforms:\n\u001b[1;32m     32\u001b[0m         image \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransforms(image)\n",
      "File \u001b[0;32m~/miniconda3/envs/fibsem/lib/python3.9/site-packages/dask/array/core.py:1689\u001b[0m, in \u001b[0;36mArray.__array__\u001b[0;34m(self, dtype, **kwargs)\u001b[0m\n\u001b[1;32m   1688\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__array__\u001b[39m(\u001b[39mself\u001b[39m, dtype\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m-> 1689\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcompute()\n\u001b[1;32m   1690\u001b[0m     \u001b[39mif\u001b[39;00m dtype \u001b[39mand\u001b[39;00m x\u001b[39m.\u001b[39mdtype \u001b[39m!=\u001b[39m dtype:\n\u001b[1;32m   1691\u001b[0m         x \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mastype(dtype)\n",
      "File \u001b[0;32m~/miniconda3/envs/fibsem/lib/python3.9/site-packages/dask/base.py:315\u001b[0m, in \u001b[0;36mDaskMethodsMixin.compute\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    291\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcompute\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    292\u001b[0m     \u001b[39m\"\"\"Compute this dask collection\u001b[39;00m\n\u001b[1;32m    293\u001b[0m \n\u001b[1;32m    294\u001b[0m \u001b[39m    This turns a lazy Dask collection into its in-memory equivalent.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    313\u001b[0m \u001b[39m    dask.base.compute\u001b[39;00m\n\u001b[1;32m    314\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 315\u001b[0m     (result,) \u001b[39m=\u001b[39m compute(\u001b[39mself\u001b[39;49m, traverse\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    316\u001b[0m     \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/miniconda3/envs/fibsem/lib/python3.9/site-packages/dask/base.py:603\u001b[0m, in \u001b[0;36mcompute\u001b[0;34m(traverse, optimize_graph, scheduler, get, *args, **kwargs)\u001b[0m\n\u001b[1;32m    600\u001b[0m     keys\u001b[39m.\u001b[39mappend(x\u001b[39m.\u001b[39m__dask_keys__())\n\u001b[1;32m    601\u001b[0m     postcomputes\u001b[39m.\u001b[39mappend(x\u001b[39m.\u001b[39m__dask_postcompute__())\n\u001b[0;32m--> 603\u001b[0m results \u001b[39m=\u001b[39m schedule(dsk, keys, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    604\u001b[0m \u001b[39mreturn\u001b[39;00m repack([f(r, \u001b[39m*\u001b[39ma) \u001b[39mfor\u001b[39;00m r, (f, a) \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(results, postcomputes)])\n",
      "File \u001b[0;32m~/miniconda3/envs/fibsem/lib/python3.9/site-packages/dask/threaded.py:89\u001b[0m, in \u001b[0;36mget\u001b[0;34m(dsk, keys, cache, num_workers, pool, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(pool, multiprocessing\u001b[39m.\u001b[39mpool\u001b[39m.\u001b[39mPool):\n\u001b[1;32m     87\u001b[0m         pool \u001b[39m=\u001b[39m MultiprocessingPoolExecutor(pool)\n\u001b[0;32m---> 89\u001b[0m results \u001b[39m=\u001b[39m get_async(\n\u001b[1;32m     90\u001b[0m     pool\u001b[39m.\u001b[39;49msubmit,\n\u001b[1;32m     91\u001b[0m     pool\u001b[39m.\u001b[39;49m_max_workers,\n\u001b[1;32m     92\u001b[0m     dsk,\n\u001b[1;32m     93\u001b[0m     keys,\n\u001b[1;32m     94\u001b[0m     cache\u001b[39m=\u001b[39;49mcache,\n\u001b[1;32m     95\u001b[0m     get_id\u001b[39m=\u001b[39;49m_thread_get_id,\n\u001b[1;32m     96\u001b[0m     pack_exception\u001b[39m=\u001b[39;49mpack_exception,\n\u001b[1;32m     97\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m     98\u001b[0m )\n\u001b[1;32m    100\u001b[0m \u001b[39m# Cleanup pools associated to dead threads\u001b[39;00m\n\u001b[1;32m    101\u001b[0m \u001b[39mwith\u001b[39;00m pools_lock:\n",
      "File \u001b[0;32m~/miniconda3/envs/fibsem/lib/python3.9/site-packages/dask/local.py:500\u001b[0m, in \u001b[0;36mget_async\u001b[0;34m(submit, num_workers, dsk, result, cache, get_id, rerun_exceptions_locally, pack_exception, raise_exception, callbacks, dumps, loads, chunksize, **kwargs)\u001b[0m\n\u001b[1;32m    498\u001b[0m \u001b[39mwhile\u001b[39;00m state[\u001b[39m\"\u001b[39m\u001b[39mwaiting\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39mor\u001b[39;00m state[\u001b[39m\"\u001b[39m\u001b[39mready\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39mor\u001b[39;00m state[\u001b[39m\"\u001b[39m\u001b[39mrunning\u001b[39m\u001b[39m\"\u001b[39m]:\n\u001b[1;32m    499\u001b[0m     fire_tasks(chunksize)\n\u001b[0;32m--> 500\u001b[0m     \u001b[39mfor\u001b[39;00m key, res_info, failed \u001b[39min\u001b[39;00m queue_get(queue)\u001b[39m.\u001b[39mresult():\n\u001b[1;32m    501\u001b[0m         \u001b[39mif\u001b[39;00m failed:\n\u001b[1;32m    502\u001b[0m             exc, tb \u001b[39m=\u001b[39m loads(res_info)\n",
      "File \u001b[0;32m~/miniconda3/envs/fibsem/lib/python3.9/site-packages/dask/local.py:137\u001b[0m, in \u001b[0;36mqueue_get\u001b[0;34m(q)\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mqueue_get\u001b[39m(q):\n\u001b[0;32m--> 137\u001b[0m     \u001b[39mreturn\u001b[39;00m q\u001b[39m.\u001b[39;49mget()\n",
      "File \u001b[0;32m~/miniconda3/envs/fibsem/lib/python3.9/queue.py:171\u001b[0m, in \u001b[0;36mQueue.get\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[39melif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    170\u001b[0m     \u001b[39mwhile\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_qsize():\n\u001b[0;32m--> 171\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnot_empty\u001b[39m.\u001b[39;49mwait()\n\u001b[1;32m    172\u001b[0m \u001b[39melif\u001b[39;00m timeout \u001b[39m<\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    173\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39mtimeout\u001b[39m\u001b[39m'\u001b[39m\u001b[39m must be a non-negative number\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/fibsem/lib/python3.9/threading.py:312\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    310\u001b[0m \u001b[39mtry\u001b[39;00m:    \u001b[39m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[1;32m    311\u001b[0m     \u001b[39mif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 312\u001b[0m         waiter\u001b[39m.\u001b[39;49macquire()\n\u001b[1;32m    313\u001b[0m         gotit \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    314\u001b[0m     \u001b[39melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "# weights and biases setup\n",
    "# wandb.init(project=\"fibsem_pipeline\", entity=\"lachieburne\")\n",
    "\n",
    "# hyperparams\n",
    "num_classes = 3\n",
    "batch_size = 1\n",
    "\n",
    "# wandb.config = {\n",
    "#     \"epochs\": 8,\n",
    "#     \"batch_size\": batch_size,\n",
    "#     \"num_classes\": num_classes\n",
    "# }\n",
    "\n",
    "################################## LOAD DATASET ##################################\n",
    "print(\n",
    "    \"\\n----------------------- Loading and Preparing Data -----------------------\"\n",
    ")\n",
    "\n",
    "data_path = \"/home/ubuntu/same_size/\"\n",
    "\n",
    "# train_data_loader, val_data_loader = preprocess_data(data_path, num_classes=num_classes, batch_size=batch_size)\n",
    "from dataset import preprocess_data\n",
    "train_data_loader, val_data_loader = preprocess_data(data_path, num_classes=num_classes, batch_size=batch_size)\n",
    "\n",
    "\n",
    "print(\"\\n----------------------- Data Preprocessing Completed -----------------------\")\n",
    "\n",
    "################################## LOAD MODEL ##################################\n",
    "print(\"\\n----------------------- Loading Model -----------------------\")\n",
    "# from smp\n",
    "model = smp.Unet(\n",
    "    encoder_name=\"resnet18\",\n",
    "    encoder_weights=\"imagenet\",\n",
    "    in_channels=1,  # grayscale images\n",
    "    classes=3,  # background, needle, lamella\n",
    ")\n",
    "\n",
    "# Use gpu for training if available else use cpu\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# load model checkpoint\n",
    "# if model_checkpoint:\n",
    "#     model.load_state_dict(torch.load(model_checkpoint, map_location=device))\n",
    "#     print(f\"Checkpoint file {model_checkpoint} loaded.\")\n",
    "\n",
    "################################## SANITY CHECK ##################################\n",
    "print(\"\\n----------------------- Begin Sanity Check -----------------------\\n\")\n",
    "\n",
    "for i in range(2):\n",
    "    # testing dataloader\n",
    "    imgs, masks = next(iter(train_data_loader))\n",
    "\n",
    "    # sanity check - model, imgs, masks\n",
    "    imgs = imgs.to(device)\n",
    "    output = model(imgs)\n",
    "    pred = decode_output(output)\n",
    "\n",
    "    print(\"imgs, masks, output\")\n",
    "    print(imgs.shape, masks.shape, output.shape)\n",
    "\n",
    "\n",
    "    img_base = imgs.detach().cpu().squeeze().numpy()[0]\n",
    "    img_rgb = np.dstack((img_base, img_base, img_base))\n",
    "    gt_base = decode_segmap(masks[0].permute(1, 2, 0).squeeze())\n",
    "\n",
    "    # wb_img = wandb.Image(img_rgb, caption=\"Input Image\")\n",
    "    # wb_gt = wandb.Image(gt_base, caption=\"Ground Truth\")\n",
    "    # wb_mask = wandb.Image(pred, caption=\"Output Mask\")\n",
    "    # wandb.log({\"image\": wb_img, \"mask\": wb_mask, \"ground_truth\": wb_gt})\n",
    "\n",
    "################################## TRAINING ##################################\n",
    "print(\"\\n----------------------- Begin Training -----------------------\\n\")\n",
    "\n",
    "# train model\n",
    "model = train_model(model, device, train_data_loader, val_data_loader, epochs = 8, DEBUG=True, WANDB=False)\n",
    "\n",
    "################################## SAVE MODEL ##################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('fibsem')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "db009ac844ba89e9b98c3ad4feadda30af1aa961aadecd3a92c2bc1ebd2c2a5c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
