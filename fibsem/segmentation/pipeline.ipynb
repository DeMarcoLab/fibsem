{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lucil\\anaconda3\\envs\\fibsem\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "\n",
    "import numpy as np\n",
    "import PIL\n",
    "import torch\n",
    "from PIL import Image\n",
    "from torch.utils.data import DataLoader, Dataset, SubsetRandomSampler\n",
    "from torchvision import transforms\n",
    "from tqdm import tqdm\n",
    "import argparse\n",
    "from datetime import datetime\n",
    "    \n",
    "# import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import segmentation_models_pytorch as smp\n",
    "import torch\n",
    "from dataset import *\n",
    "from model_utils import *\n",
    "from tqdm import tqdm\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transformations\n",
    "transformation = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.Resize((1024 // 4, 1536 // 4)),\n",
    "        transforms.ToTensor(),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SegmentationDataset(Dataset):\n",
    "    def __init__(self, images, masks, num_classes: int, transforms=None):\n",
    "        self.images = images\n",
    "        self.masks = masks\n",
    "        self.num_classes = num_classes\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.images[idx]\n",
    "\n",
    "        if self.transforms:\n",
    "            image = self.transforms(image)\n",
    "\n",
    "        mask = self.masks[idx]\n",
    "\n",
    "        # - the problem was ToTensor was destroying the class index for the labels (rounding them to 0-1)\n",
    "        # need to to transformation manually\n",
    "        mask = Image.fromarray(mask).resize(\n",
    "            (1536 // 4, 1024 // 4), resample=PIL.Image.NEAREST\n",
    "        )\n",
    "        mask = torch.tensor(np.asarray(mask)).unsqueeze(0)\n",
    "\n",
    "        return image, mask\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_images_and_masks_in_path(images_path, masks_path):\n",
    "    images = []\n",
    "    masks = []\n",
    "    sorted_img_filenames = sorted(glob.glob(images_path + \".png\"))  #[-435:]\n",
    "    sorted_mask_filenames = sorted(glob.glob(masks_path + \".png\"))  #[-435:]\n",
    "\n",
    "    for img_fname, mask_fname in tqdm(\n",
    "        list(zip(sorted_img_filenames, sorted_mask_filenames))\n",
    "    ):\n",
    "\n",
    "        image = np.asarray(Image.open(img_fname))\n",
    "        mask = np.asarray(Image.open(mask_fname))\n",
    "\n",
    "        images.append(image)\n",
    "        masks.append(mask)\n",
    "    return np.array(images), np.array(masks)\n",
    "\n",
    "\n",
    "def preprocess_data(data_path, num_classes=3, batch_size=25, val_size=0.2):\n",
    "\n",
    "    img_path = f\"{data_path}/train/**/img\"\n",
    "    label_path = f\"{data_path}/train/**/label\"\n",
    "    print(f\"Loading dataset from {img_path}\")\n",
    "\n",
    "    train_images, train_masks = load_images_and_masks_in_path(img_path, label_path)\n",
    "\n",
    "    # load dataset\n",
    "    seg_dataset = SegmentationDataset(\n",
    "        train_images, train_masks, num_classes, transforms=transformation\n",
    "    )\n",
    "\n",
    "    # train/validation splits\n",
    "    dataset_size = len(seg_dataset)\n",
    "    dataset_idx = list(range(dataset_size))\n",
    "    split_idx = int(np.floor(val_size * dataset_size))\n",
    "    train_idx = dataset_idx[split_idx:]\n",
    "    val_idx = dataset_idx[:split_idx]\n",
    "\n",
    "    train_sampler = SubsetRandomSampler(train_idx)\n",
    "    val_sampler = SubsetRandomSampler(val_idx)\n",
    "\n",
    "    train_data_loader = DataLoader(\n",
    "        seg_dataset, batch_size=batch_size, sampler=train_sampler\n",
    "    )  # shuffle=True,\n",
    "    print(f\"Train dataset has {len(train_data_loader)} batches of size {batch_size}\")\n",
    "\n",
    "    val_data_loader = DataLoader(\n",
    "        seg_dataset, batch_size=batch_size, sampler=val_sampler\n",
    "    )  # shuffle=True,\n",
    "    print(f\"Validation dataset has {len(val_data_loader)} batches of size {batch_size}\")\n",
    "\n",
    "    return train_data_loader, val_data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def save_model(model, epoch):\n",
    "#     \"\"\"Helper function for saving the model based on current time and epoch\"\"\"\n",
    "    \n",
    "#     # datetime object containing current date and time\n",
    "#     now = datetime.now()\n",
    "#     # format\n",
    "#     dt_string = now.strftime(\"%d_%m_%Y_%H_%M_%S\") + f\"_n{epoch+1:02d}\"\n",
    "#     model_save_file = f\"models/{dt_string}_model.pt\"\n",
    "#     torch.save(model.state_dict(), model_save_file)\n",
    "\n",
    "#     print(f\"Model saved to {model_save_file}\")\n",
    "\n",
    "# def train_model(model, device, train_data_loader, val_data_loader, epochs, DEBUG=False):\n",
    "#     \"\"\" Helper function for training the model \"\"\"\n",
    "#     # initialise loss function and optimizer\n",
    "#     criterion = torch.nn.CrossEntropyLoss()\n",
    "#     optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "#     total_steps = len(train_data_loader)\n",
    "#     print(f\"{epochs} epochs, {total_steps} total_steps per epoch\")\n",
    "\n",
    "#     # accounting\n",
    "#     train_losses = []\n",
    "#     val_losses = []\n",
    "\n",
    "#     # training loop\n",
    "#     for epoch in tqdm(range(epochs)):\n",
    "#         print(f\"------- Epoch {epoch+1} of {epochs}  --------\")\n",
    "        \n",
    "#         train_loss = 0\n",
    "#         val_loss = 0\n",
    "        \n",
    "#         data_loader = tqdm(train_data_loader)\n",
    "\n",
    "#         for i, (images, masks) in enumerate(data_loader):\n",
    "\n",
    "#             # set model to training mode\n",
    "#             model.train()\n",
    "\n",
    "#             # move img and mask to device, reshape mask\n",
    "#             images = images.to(device)\n",
    "#             masks = masks.type(torch.LongTensor)\n",
    "#             masks = masks.reshape(\n",
    "#                 masks.shape[0], masks.shape[2], masks.shape[3]\n",
    "#             )  # remove channel dim\n",
    "#             masks = masks.to(device)\n",
    "\n",
    "#             # forward pass\n",
    "#             outputs = model(images).type(torch.FloatTensor).to(device)\n",
    "#             loss = criterion(outputs, masks)\n",
    "\n",
    "#             # backwards pass\n",
    "#             optimizer.zero_grad()\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "\n",
    "#             # evaluation\n",
    "#             train_loss += loss.item()\n",
    "#             wandb.log({\"train_loss\": loss.item()})\n",
    "#             data_loader.set_description(f\"Train Loss: {loss.item():.04f}\")\n",
    "\n",
    "#             if i % 100 == 0:\n",
    "          \n",
    "#                 if DEBUG:\n",
    "#                     model.eval()\n",
    "#                     with torch.no_grad():\n",
    "\n",
    "#                         outputs = model(images)\n",
    "#                         output_mask = decode_output(outputs)\n",
    "                        \n",
    "#                         img_base = images.detach().cpu().squeeze().numpy()\n",
    "#                         img_rgb = np.dstack((img_base, img_base, img_base))\n",
    "#                         gt_base = decode_segmap(masks.detach().cpu().permute(1, 2, 0))\n",
    "\n",
    "#                         wb_img = wandb.Image(img_rgb, caption=\"Input Image\")\n",
    "#                         wb_gt = wandb.Image(gt_base, caption=\"Ground Truth\")\n",
    "#                         wb_mask = wandb.Image(output_mask, caption=\"Output Mask\")\n",
    "#                         wandb.log({\"image\": wb_img, \"mask\": wb_mask, \"ground_truth\": wb_gt})\n",
    "                           \n",
    "        \n",
    "#         val_loader = tqdm(val_data_loader)\n",
    "#         for i, (images, masks) in enumerate(val_loader):\n",
    "            \n",
    "#             model.eval()\n",
    "            \n",
    "#             # move img and mask to device, reshape mask\n",
    "#             images = images.to(device)\n",
    "#             masks = masks.type(torch.LongTensor)\n",
    "#             masks = masks.reshape(\n",
    "#                 masks.shape[0], masks.shape[2], masks.shape[3]\n",
    "#             )  # remove channel dim\n",
    "#             masks = masks.to(device)\n",
    "\n",
    "#             # forward pass\n",
    "#             outputs = model(images).type(torch.FloatTensor).to(device)\n",
    "#             loss = criterion(outputs, masks)\n",
    "\n",
    "#             val_loss += loss.item()\n",
    "#             wandb.log({\"val_loss\": loss.item()})\n",
    "#             val_loader.set_description(f\"Val Loss: {loss.item():.04f}\")\n",
    "\n",
    "#         train_losses.append(train_loss / len(train_data_loader))\n",
    "#         val_losses.append(val_loss / len(val_data_loader))\n",
    "\n",
    "#         # save model checkpoint\n",
    "#         save_model(model, epoch)\n",
    "\n",
    "\n",
    "#     return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, epoch):\n",
    "    \"\"\"Helper function for saving the model based on current time and epoch\"\"\"\n",
    "    \n",
    "    # datetime object containing current date and time\n",
    "    now = datetime.now()\n",
    "    # format\n",
    "    dt_string = now.strftime(\"%d_%m_%Y_%H_%M_%S\") + f\"_n{epoch+1:02d}\"\n",
    "    model_save_file = f\"models/{dt_string}_model.pt\"\n",
    "    torch.save(model.state_dict(), model_save_file)\n",
    "\n",
    "    print(f\"Model saved to {model_save_file}\")\n",
    "\n",
    "def train(model, device, data_loader, criterion, optimizer, DEBUG, WANDB):\n",
    "    data_loader = tqdm(data_loader)\n",
    "    train_loss = 0\n",
    "\n",
    "    for i, (images, masks) in enumerate(data_loader):\n",
    "        # set model to training mode\n",
    "        model.train()\n",
    "\n",
    "        # move img and mask to device, reshape mask\n",
    "        images = images.to(device)\n",
    "        masks = masks.type(torch.LongTensor)\n",
    "        masks = masks.reshape(\n",
    "            masks.shape[0], masks.shape[2], masks.shape[3]\n",
    "        )  # remove channel dim\n",
    "        masks = masks.to(device)\n",
    "\n",
    "        # forward pass\n",
    "        outputs = model(images).type(torch.FloatTensor).to(device)\n",
    "        loss = criterion(outputs, masks)\n",
    "\n",
    "        # backwards pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # evaluation\n",
    "        train_loss += loss.item()\n",
    "        if WANDB:\n",
    "            wandb.log({\"train_loss\": loss.item()})\n",
    "        data_loader.set_description(f\"Train Loss: {loss.item():.04f}\")\n",
    "\n",
    "        if i % 100 == 0:\n",
    "        \n",
    "            if DEBUG and WANDB:\n",
    "                model.eval()\n",
    "                with torch.no_grad():\n",
    "\n",
    "                    outputs = model(images)\n",
    "                    output_mask = decode_output(outputs)\n",
    "                    \n",
    "                    img_base = images.detach().cpu().squeeze().numpy()\n",
    "                    img_rgb = np.dstack((img_base, img_base, img_base))\n",
    "                    gt_base = decode_segmap(masks.detach().cpu().permute(1, 2, 0))\n",
    "\n",
    "                    wb_img = wandb.Image(img_rgb, caption=\"Input Image\")\n",
    "                    wb_gt = wandb.Image(gt_base, caption=\"Ground Truth\")\n",
    "                    wb_mask = wandb.Image(output_mask, caption=\"Output Mask\")\n",
    "                    wandb.log({\"image\": wb_img, \"mask\": wb_mask, \"ground_truth\": wb_gt})\n",
    "    \n",
    "    return train_loss\n",
    "\n",
    "def validate(model, device, data_loader, criterion, WANDB):\n",
    "    val_loader = tqdm(data_loader)\n",
    "    val_loss = 0\n",
    "\n",
    "    for i, (images, masks) in enumerate(val_loader):\n",
    "        \n",
    "        model.eval()\n",
    "        \n",
    "        # move img and mask to device, reshape mask\n",
    "        images = images.to(device)\n",
    "        masks = masks.type(torch.LongTensor)\n",
    "        masks = masks.reshape(\n",
    "            masks.shape[0], masks.shape[2], masks.shape[3]\n",
    "        )  # remove channel dim\n",
    "        masks = masks.to(device)\n",
    "\n",
    "        # forward pass\n",
    "        outputs = model(images).type(torch.FloatTensor).to(device)\n",
    "        loss = criterion(outputs, masks)\n",
    "\n",
    "        val_loss += loss.item()\n",
    "        if WANDB:\n",
    "            wandb.log({\"val_loss\": loss.item()})\n",
    "            val_loader.set_description(f\"Val Loss: {loss.item():.04f}\")\n",
    "\n",
    "    return val_loss\n",
    "\n",
    "def train_model(model, device, train_data_loader, val_data_loader, epochs, DEBUG=True, WANDB=True):\n",
    "    \"\"\" Helper function for training the model \"\"\"\n",
    "    # initialise loss function and optimizer\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "    total_steps = len(train_data_loader)\n",
    "    print(f\"{epochs} epochs, {total_steps} total_steps per epoch\")\n",
    "\n",
    "    # accounting\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "\n",
    "    # training loop\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        print(f\"------- Epoch {epoch+1} of {epochs}  --------\")\n",
    "        \n",
    "        train_loss = train(model, device, train_data_loader, criterion, optimizer, DEBUG, WANDB)\n",
    "        val_loss = validate(model, device, val_data_loader, criterion, WANDB)\n",
    "   \n",
    "        train_losses.append(train_loss / len(train_data_loader))\n",
    "        val_losses.append(val_loss / len(val_data_loader))\n",
    "\n",
    "        # save model checkpoint\n",
    "        save_model(model, epoch)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mlnae0002\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\lucil\\OneDrive\\Bureau\\DeMarco_Lab\\fibsem\\fibsem\\segmentation\\wandb\\run-20220906_140114-3exb87yu</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/lnae0002/fibsem_pipeline/runs/3exb87yu\" target=\"_blank\">devoted-sun-10</a></strong> to <a href=\"https://wandb.ai/lnae0002/fibsem_pipeline\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------------- Loading and Preparing Data -----------------------\n",
      "Loading dataset from C:\\Users\\lucil\\OneDrive\\Bureau\\DeMarco_Lab\\data/train/**/img\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1228/1228 [00:22<00:00, 54.15it/s]\n",
      "C:\\Users\\lucil\\AppData\\Local\\Temp\\ipykernel_22916\\4039661637.py:16: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return np.array(images), np.array(masks)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset has 983 batches of size 1\n",
      "Validation dataset has 245 batches of size 1\n",
      "\n",
      "----------------------- Data Preprocessing Completed -----------------------\n",
      "\n",
      "----------------------- Loading Model -----------------------\n",
      "\n",
      "----------------------- Begin Sanity Check -----------------------\n",
      "\n",
      "imgs, masks, output\n",
      "torch.Size([1, 1, 256, 384]) torch.Size([1, 1, 256, 384]) torch.Size([1, 3, 256, 384])\n",
      "imgs, masks, output\n",
      "torch.Size([1, 1, 256, 384]) torch.Size([1, 1, 256, 384]) torch.Size([1, 3, 256, 384])\n",
      "\n",
      "----------------------- Begin Training -----------------------\n",
      "\n",
      "8 epochs, 983 total_steps per epoch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/8 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- Epoch 1 of 8  --------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0375: 100%|██████████| 983/983 [01:10<00:00, 13.89it/s]\n",
      "Val Loss: 0.0847: 100%|██████████| 245/245 [00:07<00:00, 33.75it/s]\n",
      " 12%|█▎        | 1/8 [01:18<09:07, 78.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to models/06_09_2022_14_03_07_n01_model.pt\n",
      "------- Epoch 2 of 8  --------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0293: 100%|██████████| 983/983 [01:04<00:00, 15.28it/s]\n",
      "Val Loss: 0.0488: 100%|██████████| 245/245 [00:06<00:00, 36.42it/s]\n",
      " 25%|██▌       | 2/8 [02:29<07:24, 74.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to models/06_09_2022_14_04_18_n02_model.pt\n",
      "------- Epoch 3 of 8  --------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0248: 100%|██████████| 983/983 [01:03<00:00, 15.48it/s]\n",
      "Val Loss: 0.0214: 100%|██████████| 245/245 [00:06<00:00, 36.74it/s]\n",
      " 38%|███▊      | 3/8 [03:39<06:01, 72.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to models/06_09_2022_14_05_29_n03_model.pt\n",
      "------- Epoch 4 of 8  --------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0252: 100%|██████████| 983/983 [01:05<00:00, 14.90it/s]\n",
      "Val Loss: 0.0270: 100%|██████████| 245/245 [00:07<00:00, 33.22it/s]\n",
      " 50%|█████     | 4/8 [04:53<04:51, 72.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to models/06_09_2022_14_06_42_n04_model.pt\n",
      "------- Epoch 5 of 8  --------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0336: 100%|██████████| 983/983 [01:02<00:00, 15.80it/s]\n",
      "Val Loss: 0.0294: 100%|██████████| 245/245 [00:06<00:00, 38.47it/s]\n",
      " 62%|██████▎   | 5/8 [06:01<03:33, 71.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to models/06_09_2022_14_07_51_n05_model.pt\n",
      "------- Epoch 6 of 8  --------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0215: 100%|██████████| 983/983 [01:02<00:00, 15.76it/s]\n",
      "Val Loss: 0.0351: 100%|██████████| 245/245 [00:06<00:00, 37.46it/s]\n",
      " 75%|███████▌  | 6/8 [07:10<02:21, 70.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to models/06_09_2022_14_09_00_n06_model.pt\n",
      "------- Epoch 7 of 8  --------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0160: 100%|██████████| 983/983 [01:02<00:00, 15.78it/s]\n",
      "Val Loss: 0.1579: 100%|██████████| 245/245 [00:06<00:00, 38.47it/s]\n",
      " 88%|████████▊ | 7/8 [08:19<01:09, 69.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to models/06_09_2022_14_10_09_n07_model.pt\n",
      "------- Epoch 8 of 8  --------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0075: 100%|██████████| 983/983 [01:02<00:00, 15.79it/s]\n",
      "Val Loss: 0.0121: 100%|██████████| 245/245 [00:06<00:00, 38.93it/s]\n",
      "100%|██████████| 8/8 [09:28<00:00, 71.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to models/06_09_2022_14_11_17_n08_model.pt\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# weights and biases setup\n",
    "wandb.init(project=\"fibsem_pipeline\", entity=\"lnae0002\")\n",
    "\n",
    "# hyperparams\n",
    "num_classes = 3\n",
    "batch_size = 1\n",
    "\n",
    "wandb.config = {\n",
    "    \"epochs\": 8,\n",
    "    \"batch_size\": batch_size,\n",
    "    \"num_classes\": num_classes\n",
    "}\n",
    "\n",
    "################################## LOAD DATASET ##################################\n",
    "print(\n",
    "    \"\\n----------------------- Loading and Preparing Data -----------------------\"\n",
    ")\n",
    "\n",
    "data_path = r'C:\\Users\\lucil\\OneDrive\\Bureau\\DeMarco_Lab\\data'\n",
    "\n",
    "train_data_loader, val_data_loader = preprocess_data(data_path, num_classes=num_classes, batch_size=batch_size)\n",
    "\n",
    "print(\"\\n----------------------- Data Preprocessing Completed -----------------------\")\n",
    "\n",
    "################################## LOAD MODEL ##################################\n",
    "print(\"\\n----------------------- Loading Model -----------------------\")\n",
    "# from smp\n",
    "model = smp.Unet(\n",
    "    encoder_name=\"resnet18\",\n",
    "    encoder_weights=\"imagenet\",\n",
    "    in_channels=1,  # grayscale images\n",
    "    classes=3,  # background, needle, lamella\n",
    ")\n",
    "\n",
    "# Use gpu for training if available else use cpu\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# load model checkpoint\n",
    "# if model_checkpoint:\n",
    "#     model.load_state_dict(torch.load(model_checkpoint, map_location=device))\n",
    "#     print(f\"Checkpoint file {model_checkpoint} loaded.\")\n",
    "\n",
    "################################## SANITY CHECK ##################################\n",
    "print(\"\\n----------------------- Begin Sanity Check -----------------------\\n\")\n",
    "\n",
    "for i in range(2):\n",
    "    # testing dataloader\n",
    "    imgs, masks = next(iter(train_data_loader))\n",
    "\n",
    "    # sanity check - model, imgs, masks\n",
    "    imgs = imgs.to(device)\n",
    "    output = model(imgs)\n",
    "    pred = decode_output(output)\n",
    "\n",
    "    print(\"imgs, masks, output\")\n",
    "    print(imgs.shape, masks.shape, output.shape)\n",
    "\n",
    "\n",
    "    img_base = imgs.detach().cpu().squeeze().numpy()[0]\n",
    "    img_rgb = np.dstack((img_base, img_base, img_base))\n",
    "    gt_base = decode_segmap(masks[0].permute(1, 2, 0).squeeze())\n",
    "\n",
    "    wb_img = wandb.Image(img_rgb, caption=\"Input Image\")\n",
    "    wb_gt = wandb.Image(gt_base, caption=\"Ground Truth\")\n",
    "    wb_mask = wandb.Image(pred, caption=\"Output Mask\")\n",
    "    wandb.log({\"image\": wb_img, \"mask\": wb_mask, \"ground_truth\": wb_gt})\n",
    "\n",
    "################################## TRAINING ##################################\n",
    "print(\"\\n----------------------- Begin Training -----------------------\\n\")\n",
    "\n",
    "# train model\n",
    "model = train_model(model, device, train_data_loader, val_data_loader, epochs = 8, DEBUG=True)\n",
    "\n",
    "################################## SAVE MODEL ##################################"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "dfd385977d9ccdc365b7fa138fb5baebfcc47440065f7b0362d539c2851e81a2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
