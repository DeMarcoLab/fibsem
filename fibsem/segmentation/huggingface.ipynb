{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "food = load_dataset(\"food101\", split=\"train[:5000]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "food = food.train_test_split(test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "food[\"train\"][0]\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(food[\"train\"][0][\"image\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = food[\"train\"].features[\"label\"].names\n",
    "label2id, id2label = dict(), dict()\n",
    "for i, label in enumerate(labels):\n",
    "    label2id[label] = str(i)\n",
    "    id2label[str(i)] = label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2label[str(79)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoImageProcessor\n",
    "\n",
    "checkpoint = \"google/vit-base-patch16-224-in21k\"\n",
    "image_processor = AutoImageProcessor.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import RandomResizedCrop, Compose, Normalize, ToTensor\n",
    "\n",
    "normalize = Normalize(mean=image_processor.image_mean, std=image_processor.image_std)\n",
    "size = (\n",
    "    image_processor.size[\"shortest_edge\"]\n",
    "    if \"shortest_edge\" in image_processor.size\n",
    "    else (image_processor.size[\"height\"], image_processor.size[\"width\"])\n",
    ")\n",
    "_transforms = Compose([RandomResizedCrop(size), ToTensor(), normalize])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transforms(examples):\n",
    "    examples[\"pixel_values\"] = [_transforms(img.convert(\"RGB\")) for img in examples[\"image\"]]\n",
    "    del examples[\"image\"]\n",
    "    return examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "food = food.with_transform(transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DefaultDataCollator\n",
    "\n",
    "data_collator = DefaultDataCollator()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "accuracy = evaluate.load(\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return accuracy.compute(predictions=predictions, references=labels) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForImageClassification, TrainingArguments, Trainer\n",
    "\n",
    "model = AutoModelForImageClassification.from_pretrained(\n",
    "    checkpoint,\n",
    "    num_labels=len(labels),\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"my_awesome_food_model\",\n",
    "    remove_unused_columns=False,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    gradient_accumulation_steps=4,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    warmup_ratio=0.1,\n",
    "    logging_steps=10,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\",\n",
    "    push_to_hub=False,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=food[\"train\"],\n",
    "    eval_dataset=food[\"test\"],\n",
    "    tokenizer=image_processor,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = load_dataset(\"food101\", split=\"validation[:10]\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "import random\n",
    "idx = random.randint(0, len(ds[\"image\"])- 1)\n",
    "image = ds[\"image\"][idx]\n",
    "\n",
    "plt.imshow(image)\n",
    "plt.show()\n",
    "\n",
    "classifier = pipeline(\"image-classification\", model=\"my_awesome_food_model/checkpoint-186\")\n",
    "classifier(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SegFormer - AutoLamella\n",
    "\n",
    "\n",
    "TODO:\n",
    "- Add id2label.json to dataset repo\n",
    "- Add inference pipeline to model.py\n",
    "- Migrate to stand alone script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, concatenate_datasets\n",
    "\n",
    "waffle_train_ds = load_dataset(\"patrickcleeve/autolamella\", name=\"waffle\", split=\"train\")\n",
    "liftout_train_ds = load_dataset(\"patrickcleeve/autolamella\", name=\"liftout\", split=\"train\")\n",
    "serial_liftout_train_ds = load_dataset(\"patrickcleeve/autolamella\", name=\"serial-liftout\", split=\"train\")\n",
    "\n",
    "\n",
    "waffle_test_ds = load_dataset(\"patrickcleeve/autolamella\", name=\"waffle\", split=\"test\")\n",
    "liftout_test_ds = load_dataset(\"patrickcleeve/autolamella\", name=\"liftout\", split=\"test\")\n",
    "serial_liftout_test_ds = load_dataset(\"patrickcleeve/autolamella\", name=\"serial-liftout\", split=\"test\")\n",
    "\n",
    "# # concatenate datasets (e.g. mega model)\n",
    "train_ds = concatenate_datasets([waffle_train_ds, liftout_train_ds, serial_liftout_train_ds])\n",
    "test_ds = concatenate_datasets([waffle_test_ds, liftout_test_ds, serial_liftout_test_ds], split=\"test\")\n",
    "\n",
    "# ds = load_dataset(\"patrickcleeve/autolamella\", name=\"waffle\")\n",
    "\n",
    "# train_ds = ds[\"train\"]\n",
    "# test_ds = ds[\"test\"]\n",
    "\n",
    "print(len(train_ds))\n",
    "print(len(test_ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "idx = random.randint(0, len(train_ds) - 1)\n",
    "\n",
    "image = np.asarray(Image.fromarray(np.asarray(train_ds[idx][\"image\"])).convert(\"RGB\"))\n",
    "labels = train_ds[idx][\"annotation\"]\n",
    "# image = image.transpose(1, 2, 0)\n",
    "\n",
    "print(image.shape)\n",
    "plt.imshow(image)\n",
    "plt.imshow(labels, alpha=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "id2label = {0: \"background\", 1: \"lamella\", 2: \"manipulator\", 3: \"landing_post\", 4: \"copper_adapter\", 5: \"volume_block\"}\n",
    "label2id = {v: k for k, v in id2label.items()}\n",
    "\n",
    "num_labels = len(id2label)\n",
    "print(id2label, num_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import ColorJitter, ToPILImage\n",
    "from transformers import SegformerImageProcessor\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def to_rgb(image):\n",
    "    # TODO: surely a better way to do this\n",
    "    return np.asarray(Image.fromarray(np.asarray(image)).convert(\"RGB\"))\n",
    "\n",
    "processor = SegformerImageProcessor(do_resize=True, size={\"height\": 512, \"width\": 768})\n",
    "jitter = ColorJitter(brightness=0.25, contrast=0.25, saturation=0.25, hue=0.1) \n",
    "\n",
    "def train_transforms(example_batch):\n",
    "    images = [to_rgb(x) for x in example_batch['image']]\n",
    "    labels = [x for x in example_batch['annotation']]\n",
    "    inputs = processor(images, labels)\n",
    "    return inputs\n",
    "\n",
    "\n",
    "def val_transforms(example_batch):\n",
    "    images = [to_rgb(x) for x in example_batch['image']]\n",
    "    labels = [x for x in example_batch['annotation']]\n",
    "    inputs = processor(images, labels)\n",
    "    return inputs\n",
    "\n",
    "\n",
    "# Set transforms\n",
    "train_ds.set_transform(train_transforms)\n",
    "test_ds.set_transform(val_transforms)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import random\n",
    "idx = random.randint(0, len(train_ds) - 1)\n",
    "image = train_ds[idx][\"pixel_values\"]\n",
    "labels = train_ds[idx][\"labels\"]\n",
    "image = image.transpose(1, 2, 0)\n",
    "plt.imshow(image, cmap=\"gray\")\n",
    "plt.imshow(labels, alpha=0.5)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import SegformerForSemanticSegmentation\n",
    "\n",
    "pretrained_model_name = \"nvidia/mit-b1\" \n",
    "model = SegformerForSemanticSegmentation.from_pretrained(\n",
    "    pretrained_model_name,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "epochs = 50\n",
    "lr = 0.00006\n",
    "batch_size = 2\n",
    "\n",
    "hub_model_id = \"segformer-b0-finetuned-autolamella-mega-1\"\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    hub_model_id,\n",
    "    learning_rate=lr,\n",
    "    num_train_epochs=epochs,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    save_total_limit=3,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=20,\n",
    "    eval_steps=20,\n",
    "    logging_steps=1,\n",
    "    eval_accumulation_steps=5,\n",
    "    load_best_model_at_end=True,\n",
    "    push_to_hub=False,\n",
    "    hub_model_id=hub_model_id,\n",
    "    # hub_strategy=\"end\",\n",
    "    report_to=\"wandb\",\n",
    "    run_name=hub_model_id,\n",
    "    remove_unused_columns=False,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"mean_iou\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "  with torch.no_grad():\n",
    "    logits, labels = eval_pred\n",
    "    logits_tensor = torch.from_numpy(logits)\n",
    "    # scale the logits to the size of the label\n",
    "    logits_tensor = nn.functional.interpolate(\n",
    "        logits_tensor,\n",
    "        size=labels.shape[-2:],\n",
    "        mode=\"bilinear\",\n",
    "        align_corners=False,\n",
    "    ).argmax(dim=1)\n",
    "\n",
    "    pred_labels = logits_tensor.detach().cpu().numpy()\n",
    "    # currently using _compute instead of compute\n",
    "    # see this issue for more info: https://github.com/huggingface/evaluate/pull/328#issuecomment-1286866576\n",
    "    metrics = metric._compute(\n",
    "            predictions=pred_labels,\n",
    "            references=labels,\n",
    "            num_labels=len(id2label),\n",
    "            ignore_index=0,\n",
    "            reduce_labels=processor.do_reduce_labels,\n",
    "        )\n",
    "    \n",
    "    # add per category metrics as individual key-value pairs\n",
    "    per_category_accuracy = metrics.pop(\"per_category_accuracy\").tolist()\n",
    "    per_category_iou = metrics.pop(\"per_category_iou\").tolist()\n",
    "\n",
    "    metrics.update({f\"accuracy_{id2label[i]}\": v for i, v in enumerate(per_category_accuracy)})\n",
    "    metrics.update({f\"iou_{id2label[i]}\": v for i, v in enumerate(per_category_iou)})\n",
    "    \n",
    "    return metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=test_ds,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kwargs = {\n",
    "    \"tags\": [\"vision\", \"image-segmentation\"],\n",
    "    \"finetuned_from\": pretrained_model_name,\n",
    "    \"dataset\": \"patrickcleeve/autolamella\",\n",
    "}\n",
    "\n",
    "processor.push_to_hub(hub_model_id)\n",
    "trainer.push_to_hub(**kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import SegformerImageProcessor, SegformerForSemanticSegmentation\n",
    "\n",
    "hf_username = \"patrickcleeve\"\n",
    "hub_model_id = \"segformer-b1-autolamella-mega-1\"\n",
    "# processor = SegformerImageProcessor.from_pretrained(\"nvidia/segformer-b0-finetuned-ade-512-512\")\n",
    "model = SegformerForSemanticSegmentation.from_pretrained(f\"{hf_username}/{hub_model_id}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "from fibsem.segmentation.utils import decode_segmap_v2\n",
    "# image = test_ds[0]['pixel_values']\n",
    "# gt_seg = test_ds[0]\n",
    "# image\n",
    "\n",
    "\n",
    "# plt.imshow(image, cmap=\"gray\")\n",
    "# plt.imshow(gt_seg, alpha=0.5)\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "ds1 = load_dataset(\"patrickcleeve/autolamella\", name=\"waffle\", split=\"test\")\n",
    "ds2 = load_dataset(\"patrickcleeve/autolamella\", name=\"liftout\", split=\"test\")\n",
    "ds3 = load_dataset(\"patrickcleeve/autolamella\", name=\"serial-liftout\", split=\"test\")\n",
    "\n",
    "ds = concatenate_datasets([ds1, ds2, ds3])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for i in range(100):\n",
    "\n",
    "    idx = random.randint(0, len(ds) - 1)\n",
    "\n",
    "    image = ds[idx]['image']\n",
    "    gt_seg = np.asarray(ds[idx]['annotation'])\n",
    "\n",
    "    image = np.asarray(Image.fromarray(np.asarray(image)).convert(\"RGB\"))\n",
    "\n",
    "\n",
    "    # plt.imshow(image)\n",
    "    # plt.imshow(decode_segmap_v2(gt_seg), alpha=0.5)\n",
    "    # plt.show()\n",
    "\n",
    "\n",
    "    processor = SegformerImageProcessor.from_pretrained(f\"{hf_username}/{hub_model_id}\")\n",
    "\n",
    "    inputs = processor(images=image, return_tensors=\"pt\")\n",
    "    outputs = model(**inputs)\n",
    "    logits = outputs.logits  # shape (batch_size, num_labels, height/4, width/4)\n",
    "\n",
    "    print(inputs[\"pixel_values\"].shape)\n",
    "\n",
    "    # First, rescale logits to original image size\n",
    "    upsampled_logits = nn.functional.interpolate(\n",
    "        logits,\n",
    "        size=(1024, 1536), # (height, width)\n",
    "        mode='bilinear',\n",
    "        align_corners=False\n",
    "    )\n",
    "\n",
    "    # Second, apply argmax on the class dimension\n",
    "    pred_seg = upsampled_logits.argmax(dim=1)[0]\n",
    "\n",
    "    # plot the prediction and ground truth\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    plt.suptitle(f\"Image {idx}\")\n",
    "    ax[0].imshow(image)\n",
    "    ax[0].imshow(decode_segmap_v2(gt_seg), alpha=0.5)\n",
    "    ax[0].axis('off')\n",
    "    ax[0].set_title('Image')\n",
    "    ax[1].imshow(image)\n",
    "    ax[1].imshow(decode_segmap_v2(pred_seg), alpha=0.5)\n",
    "    ax[1].set_title('Prediction')\n",
    "    ax[1].axis('off')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from fibsem.segmentation.hf_segmentation_model import SegmentationModelHuggingFace\n",
    "from fibsem.segmentation.model import load_model\n",
    "from fibsem.segmentation.utils import decode_segmap_v2\n",
    "from datasets import load_dataset, concatenate_datasets\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "\n",
    "ds1 = load_dataset(\"patrickcleeve/autolamella\", name=\"waffle\", split=\"test\")\n",
    "ds2 = load_dataset(\"patrickcleeve/autolamella\", name=\"liftout\", split=\"test\")\n",
    "ds3 = load_dataset(\"patrickcleeve/autolamella\", name=\"serial-liftout\", split=\"test\")\n",
    "\n",
    "ds = concatenate_datasets([ds1, ds2, ds3])\n",
    "\n",
    "checkpoint = \"patrickcleeve/segformer-b1-autolamella-mega-1\"\n",
    "model = load_model(checkpoint)\n",
    "\n",
    "\n",
    "for i in range(20):\n",
    "\n",
    "    idx = random.randint(0, len(ds) - 1)\n",
    "\n",
    "    image = np.asarray(ds[idx]['image'])\n",
    "    gt_seg = np.asarray(ds[idx]['annotation'])\n",
    "\n",
    "    masks = model.inference(image, rgb=False)\n",
    "\n",
    "\n",
    "    # plot the prediction and ground truth\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(12, 5))\n",
    "    plt.suptitle(f\"Image {idx}\")\n",
    "    ax[0].imshow(image, cmap=\"gray\")\n",
    "    ax[0].imshow(decode_segmap_v2(gt_seg), alpha=0.5)\n",
    "    ax[0].axis('off')\n",
    "    ax[0].set_title('Image')\n",
    "    ax[1].imshow(image, cmap=\"gray\")\n",
    "    ax[1].imshow(decode_segmap_v2(masks), alpha=0.5)\n",
    "    ax[1].set_title('Prediction')\n",
    "    ax[1].axis('off')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fibsem",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
