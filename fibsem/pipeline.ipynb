{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lucil\\anaconda3\\envs\\fibsem\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "\n",
    "import numpy as np\n",
    "import PIL\n",
    "import torch\n",
    "from PIL import Image\n",
    "from torch.utils.data import DataLoader, Dataset, SubsetRandomSampler\n",
    "from torchvision import transforms\n",
    "from tqdm import tqdm\n",
    "import argparse\n",
    "from datetime import datetime\n",
    "    \n",
    "# import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import segmentation_models_pytorch as smp\n",
    "import torch\n",
    "from dataset import *\n",
    "from model_utils import *\n",
    "from tqdm import tqdm\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transformations\n",
    "transformation = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.Resize((1024 // 4, 1536 // 4)),\n",
    "        transforms.ToTensor(),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SegmentationDataset(Dataset):\n",
    "    def __init__(self, images, masks, num_classes: int, transforms=None):\n",
    "        self.images = images\n",
    "        self.masks = masks\n",
    "        self.num_classes = num_classes\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.images[idx]\n",
    "\n",
    "        if self.transforms:\n",
    "            image = self.transforms(image)\n",
    "\n",
    "        mask = self.masks[idx]\n",
    "\n",
    "        # - the problem was ToTensor was destroying the class index for the labels (rounding them to 0-1)\n",
    "        # need to to transformation manually\n",
    "        mask = Image.fromarray(mask).resize(\n",
    "            (1536 // 4, 1024 // 4), resample=PIL.Image.NEAREST\n",
    "        )\n",
    "        mask = torch.tensor(np.asarray(mask)).unsqueeze(0)\n",
    "\n",
    "        return image, mask\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_images_and_masks_in_path(images_path, masks_path):\n",
    "    images = []\n",
    "    masks = []\n",
    "    sorted_img_filenames = sorted(glob.glob(images_path + \".png\"))  #[-435:]\n",
    "    sorted_mask_filenames = sorted(glob.glob(masks_path + \".png\"))  #[-435:]\n",
    "\n",
    "    for img_fname, mask_fname in tqdm(\n",
    "        list(zip(sorted_img_filenames, sorted_mask_filenames))\n",
    "    ):\n",
    "\n",
    "        image = np.asarray(Image.open(img_fname))\n",
    "        mask = np.asarray(Image.open(mask_fname))\n",
    "\n",
    "        images.append(image)\n",
    "        masks.append(mask)\n",
    "    return np.array(images), np.array(masks)\n",
    "\n",
    "\n",
    "def preprocess_data(data_path, num_classes=3, batch_size=25, val_size=0.2):\n",
    "\n",
    "    img_path = f\"{data_path}/train/**/img\"\n",
    "    label_path = f\"{data_path}/train/**/label\"\n",
    "    print(f\"Loading dataset from {img_path}\")\n",
    "\n",
    "    train_images, train_masks = load_images_and_masks_in_path(img_path, label_path)\n",
    "\n",
    "    # load dataset\n",
    "    seg_dataset = SegmentationDataset(\n",
    "        train_images, train_masks, num_classes, transforms=transformation\n",
    "    )\n",
    "\n",
    "    # train/validation splits\n",
    "    dataset_size = len(seg_dataset)\n",
    "    dataset_idx = list(range(dataset_size))\n",
    "    split_idx = int(np.floor(val_size * dataset_size))\n",
    "    train_idx = dataset_idx[split_idx:]\n",
    "    val_idx = dataset_idx[:split_idx]\n",
    "\n",
    "    train_sampler = SubsetRandomSampler(train_idx)\n",
    "    val_sampler = SubsetRandomSampler(val_idx)\n",
    "\n",
    "    train_data_loader = DataLoader(\n",
    "        seg_dataset, batch_size=batch_size, sampler=train_sampler\n",
    "    )  # shuffle=True,\n",
    "    print(f\"Train dataset has {len(train_data_loader)} batches of size {batch_size}\")\n",
    "\n",
    "    val_data_loader = DataLoader(\n",
    "        seg_dataset, batch_size=batch_size, sampler=val_sampler\n",
    "    )  # shuffle=True,\n",
    "    print(f\"Validation dataset has {len(val_data_loader)} batches of size {batch_size}\")\n",
    "\n",
    "    return train_data_loader, val_data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, epoch):\n",
    "    \"\"\"Helper function for saving the model based on current time and epoch\"\"\"\n",
    "    \n",
    "    # datetime object containing current date and time\n",
    "    now = datetime.now()\n",
    "    # format\n",
    "    dt_string = now.strftime(\"%d_%m_%Y_%H_%M_%S\") + f\"_n{epoch+1:02d}\"\n",
    "    model_save_file = f\"models/{dt_string}_model.pt\"\n",
    "    torch.save(model.state_dict(), model_save_file)\n",
    "\n",
    "    print(f\"Model saved to {model_save_file}\")\n",
    "\n",
    "def train_model(model, device, train_data_loader, val_data_loader, epochs, DEBUG=False):\n",
    "    \"\"\" Helper function for training the model \"\"\"\n",
    "    # initialise loss function and optimizer\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "    total_steps = len(train_data_loader)\n",
    "    print(f\"{epochs} epochs, {total_steps} total_steps per epoch\")\n",
    "\n",
    "    # accounting\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "\n",
    "    # training loop\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        print(f\"------- Epoch {epoch+1} of {epochs}  --------\")\n",
    "        \n",
    "        train_loss = 0\n",
    "        val_loss = 0\n",
    "        \n",
    "        data_loader = tqdm(train_data_loader)\n",
    "\n",
    "        for i, (images, masks) in enumerate(data_loader):\n",
    "\n",
    "            # set model to training mode\n",
    "            model.train()\n",
    "\n",
    "            # move img and mask to device, reshape mask\n",
    "            images = images.to(device)\n",
    "            masks = masks.type(torch.LongTensor)\n",
    "            masks = masks.reshape(\n",
    "                masks.shape[0], masks.shape[2], masks.shape[3]\n",
    "            )  # remove channel dim\n",
    "            masks = masks.to(device)\n",
    "\n",
    "            # forward pass\n",
    "            outputs = model(images).type(torch.FloatTensor).to(device)\n",
    "            loss = criterion(outputs, masks)\n",
    "\n",
    "            # backwards pass\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # evaluation\n",
    "            train_loss += loss.item()\n",
    "            wandb.log({\"train_loss\": loss.item()})\n",
    "            data_loader.set_description(f\"Train Loss: {loss.item():.04f}\")\n",
    "\n",
    "            if i % 100 == 0:\n",
    "          \n",
    "                if DEBUG:\n",
    "                    model.eval()\n",
    "                    with torch.no_grad():\n",
    "\n",
    "                        outputs = model(images)\n",
    "                        output_mask = decode_output(outputs)\n",
    "                        \n",
    "                        img_base = images.detach().cpu().squeeze().numpy()\n",
    "                        img_rgb = np.dstack((img_base, img_base, img_base))\n",
    "                        gt_base = decode_segmap(masks.detach().cpu().permute(1, 2, 0))\n",
    "\n",
    "                        wb_img = wandb.Image(img_rgb, caption=\"Input Image\")\n",
    "                        wb_gt = wandb.Image(gt_base, caption=\"Ground Truth\")\n",
    "                        wb_mask = wandb.Image(output_mask, caption=\"Output Mask\")\n",
    "                        wandb.log({\"image\": wb_img, \"mask\": wb_mask, \"ground_truth\": wb_gt})\n",
    "                           \n",
    "        \n",
    "        val_loader = tqdm(val_data_loader)\n",
    "        for i, (images, masks) in enumerate(val_loader):\n",
    "            \n",
    "            model.eval()\n",
    "            \n",
    "            # move img and mask to device, reshape mask\n",
    "            images = images.to(device)\n",
    "            masks = masks.type(torch.LongTensor)\n",
    "            masks = masks.reshape(\n",
    "                masks.shape[0], masks.shape[2], masks.shape[3]\n",
    "            )  # remove channel dim\n",
    "            masks = masks.to(device)\n",
    "\n",
    "            # forward pass\n",
    "            outputs = model(images).type(torch.FloatTensor).to(device)\n",
    "            loss = criterion(outputs, masks)\n",
    "\n",
    "            val_loss += loss.item()\n",
    "            wandb.log({\"val_loss\": loss.item()})\n",
    "            val_loader.set_description(f\"Val Loss: {loss.item():.04f}\")\n",
    "\n",
    "        train_losses.append(train_loss / len(train_data_loader))\n",
    "        val_losses.append(val_loss / len(val_data_loader))\n",
    "\n",
    "        # save model checkpoint\n",
    "        save_model(model, epoch)\n",
    "\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:1s1eo2eq) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>train_loss</td><td>█▃▂▂▁▁▁▁▂▂▂█▁▁▁▁▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>▁▁▁▂▁▁▁▁▁▁▁▁▂▁▁▁▂▄▁▁▇▄█▇▃▁▁▁▁▂▁▁▁▁▁▁▂▂▃▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>train_loss</td><td>0.01154</td></tr><tr><td>val_loss</td><td>0.01341</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">generous-surf-6</strong>: <a href=\"https://wandb.ai/lnae0002/fibsem_pipeline/runs/1s1eo2eq\" target=\"_blank\">https://wandb.ai/lnae0002/fibsem_pipeline/runs/1s1eo2eq</a><br/>Synced 5 W&B file(s), 246 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20220905_130743-1s1eo2eq\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:1s1eo2eq). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.13.2 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\lucil\\OneDrive\\Bureau\\DeMarco_Lab\\fibsem\\fibsem\\wandb\\run-20220905_132351-3f8cglaa</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/lnae0002/fibsem_pipeline/runs/3f8cglaa\" target=\"_blank\">solar-capybara-7</a></strong> to <a href=\"https://wandb.ai/lnae0002/fibsem_pipeline\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------------- Loading and Preparing Data -----------------------\n",
      "Loading dataset from C:\\Users\\lucil\\OneDrive\\Bureau\\DeMarco_Lab\\data/train/**/img\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1228/1228 [00:25<00:00, 47.49it/s]\n",
      "C:\\Users\\lucil\\AppData\\Local\\Temp\\ipykernel_4204\\4039661637.py:16: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return np.array(images), np.array(masks)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset has 983 batches of size 1\n",
      "Validation dataset has 245 batches of size 1\n",
      "\n",
      "----------------------- Data Preprocessing Completed -----------------------\n",
      "\n",
      "----------------------- Loading Model -----------------------\n",
      "\n",
      "----------------------- Begin Sanity Check -----------------------\n",
      "\n",
      "imgs, masks, output\n",
      "torch.Size([1, 1, 256, 384]) torch.Size([1, 1, 256, 384]) torch.Size([1, 3, 256, 384])\n",
      "imgs, masks, output\n",
      "torch.Size([1, 1, 256, 384]) torch.Size([1, 1, 256, 384]) torch.Size([1, 3, 256, 384])\n",
      "\n",
      "----------------------- Begin Training -----------------------\n",
      "\n",
      "8 epochs, 983 total_steps per epoch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/8 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- Epoch 1 of 8  --------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0465: 100%|██████████| 983/983 [01:05<00:00, 14.92it/s]\n",
      "Val Loss: 0.0731: 100%|██████████| 245/245 [00:06<00:00, 37.25it/s]\n",
      " 12%|█▎        | 1/8 [01:12<08:28, 72.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to models/05_09_2022_13_25_40_n01_model.pt\n",
      "------- Epoch 2 of 8  --------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0402: 100%|██████████| 983/983 [01:04<00:00, 15.31it/s]\n",
      "Val Loss: 0.0677: 100%|██████████| 245/245 [00:06<00:00, 39.20it/s]\n",
      " 25%|██▌       | 2/8 [02:23<07:08, 71.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to models/05_09_2022_13_26_50_n02_model.pt\n",
      "------- Epoch 3 of 8  --------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0214: 100%|██████████| 983/983 [01:03<00:00, 15.43it/s]\n",
      "Val Loss: 0.0741: 100%|██████████| 245/245 [00:06<00:00, 38.50it/s]\n",
      " 38%|███▊      | 3/8 [03:33<05:54, 70.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to models/05_09_2022_13_28_00_n03_model.pt\n",
      "------- Epoch 4 of 8  --------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0107: 100%|██████████| 983/983 [01:02<00:00, 15.73it/s]\n",
      "Val Loss: 0.1822: 100%|██████████| 245/245 [00:06<00:00, 38.36it/s]\n",
      " 50%|█████     | 4/8 [04:42<04:40, 70.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to models/05_09_2022_13_29_09_n04_model.pt\n",
      "------- Epoch 5 of 8  --------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0177: 100%|██████████| 983/983 [01:03<00:00, 15.45it/s]\n",
      "Val Loss: 0.0449: 100%|██████████| 245/245 [00:06<00:00, 38.48it/s]\n",
      " 62%|██████▎   | 5/8 [05:52<03:30, 70.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to models/05_09_2022_13_30_19_n05_model.pt\n",
      "------- Epoch 6 of 8  --------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0077: 100%|██████████| 983/983 [01:01<00:00, 15.88it/s]\n",
      "Val Loss: 0.0089: 100%|██████████| 245/245 [00:06<00:00, 37.35it/s]\n",
      " 75%|███████▌  | 6/8 [07:01<02:19, 69.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to models/05_09_2022_13_31_28_n06_model.pt\n",
      "------- Epoch 7 of 8  --------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.7879: 100%|██████████| 983/983 [01:03<00:00, 15.42it/s]\n",
      "Val Loss: 0.0227: 100%|██████████| 245/245 [00:06<00:00, 37.17it/s]\n",
      " 88%|████████▊ | 7/8 [08:11<01:09, 69.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to models/05_09_2022_13_32_38_n07_model.pt\n",
      "------- Epoch 8 of 8  --------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0201: 100%|██████████| 983/983 [01:04<00:00, 15.27it/s]\n",
      "Val Loss: 0.0107: 100%|██████████| 245/245 [00:06<00:00, 37.32it/s]\n",
      "100%|██████████| 8/8 [09:22<00:00, 70.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to models/05_09_2022_13_33_50_n08_model.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# weights and biases setup\n",
    "wandb.init(project=\"fibsem_pipeline\", entity=\"lnae0002\")\n",
    "\n",
    "# hyperparams\n",
    "num_classes = 3\n",
    "batch_size = 1\n",
    "\n",
    "wandb.config = {\n",
    "    \"epochs\": 8,\n",
    "    \"batch_size\": batch_size,\n",
    "    \"num_classes\": num_classes\n",
    "}\n",
    "\n",
    "################################## LOAD DATASET ##################################\n",
    "print(\n",
    "    \"\\n----------------------- Loading and Preparing Data -----------------------\"\n",
    ")\n",
    "\n",
    "data_path = r'C:\\Users\\lucil\\OneDrive\\Bureau\\DeMarco_Lab\\data'\n",
    "\n",
    "train_data_loader, val_data_loader = preprocess_data(data_path, num_classes=num_classes, batch_size=batch_size)\n",
    "\n",
    "print(\"\\n----------------------- Data Preprocessing Completed -----------------------\")\n",
    "\n",
    "################################## LOAD MODEL ##################################\n",
    "print(\"\\n----------------------- Loading Model -----------------------\")\n",
    "# from smp\n",
    "model = smp.Unet(\n",
    "    encoder_name=\"resnet18\",\n",
    "    encoder_weights=\"imagenet\",\n",
    "    in_channels=1,  # grayscale images\n",
    "    classes=3,  # background, needle, lamella\n",
    ")\n",
    "\n",
    "# Use gpu for training if available else use cpu\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# load model checkpoint\n",
    "# if model_checkpoint:\n",
    "#     model.load_state_dict(torch.load(model_checkpoint, map_location=device))\n",
    "#     print(f\"Checkpoint file {model_checkpoint} loaded.\")\n",
    "\n",
    "################################## SANITY CHECK ##################################\n",
    "print(\"\\n----------------------- Begin Sanity Check -----------------------\\n\")\n",
    "\n",
    "for i in range(2):\n",
    "    # testing dataloader\n",
    "    imgs, masks = next(iter(train_data_loader))\n",
    "\n",
    "    # sanity check - model, imgs, masks\n",
    "    imgs = imgs.to(device)\n",
    "    output = model(imgs)\n",
    "    pred = decode_output(output)\n",
    "\n",
    "    print(\"imgs, masks, output\")\n",
    "    print(imgs.shape, masks.shape, output.shape)\n",
    "\n",
    "\n",
    "    img_base = imgs.detach().cpu().squeeze().numpy()[0]\n",
    "    img_rgb = np.dstack((img_base, img_base, img_base))\n",
    "    gt_base = decode_segmap(masks[0].permute(1, 2, 0).squeeze())\n",
    "\n",
    "    wb_img = wandb.Image(img_rgb, caption=\"Input Image\")\n",
    "    wb_gt = wandb.Image(gt_base, caption=\"Ground Truth\")\n",
    "    wb_mask = wandb.Image(pred, caption=\"Output Mask\")\n",
    "    wandb.log({\"image\": wb_img, \"mask\": wb_mask, \"ground_truth\": wb_gt})\n",
    "\n",
    "################################## TRAINING ##################################\n",
    "print(\"\\n----------------------- Begin Training -----------------------\\n\")\n",
    "\n",
    "# train model\n",
    "model = train_model(model, device, train_data_loader, val_data_loader, epochs = 8, DEBUG=True)\n",
    "\n",
    "################################## SAVE MODEL ##################################"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "dfd385977d9ccdc365b7fa138fb5baebfcc47440065f7b0362d539c2851e81a2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
